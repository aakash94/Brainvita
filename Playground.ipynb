{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Env import Env\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment='__')\n",
    "tag_reward = \"reward\"\n",
    "tag_loss = \"loss\"\n",
    "tag_ep = \"epsilon\"\n",
    "r_buff_header = ['state', 'action', 'next_state', 'reward', 'done']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA      = 0.9 # discount factor\n",
    "EPSILON    = 1\n",
    "ESUB       = 0.01\n",
    "EMIN       = 0.025\n",
    "LEARN_RATE = 0.001\n",
    "EDECAY = 0.99\n",
    "\n",
    "STATE_N  = Env.DIM * Env.DIM\n",
    "ACTION_N = Env.ACTION_N\n",
    "\n",
    "NUM_EPISODES = 100000\n",
    "\n",
    "MINREWARD = 25\n",
    "MINREWARD_INCREMENT = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.header = r_buff_header\n",
    "        self.buffer = pd.DataFrame(columns=self.header)        \n",
    "\n",
    "    def push(self, df_row):\n",
    "        if self.__len__() == self.capacity:\n",
    "            # Probably exceeded capacity\n",
    "            #remove a row (probably 1st one) here \n",
    "            self.buffer = self.buffer.iloc[1:]\n",
    "        #add to dataframe here\n",
    "        self.buffer = pd.concat([self.buffer, df_row])\n",
    "        \n",
    "        \n",
    "    def insert(self, stateV, actonV, next_stateV, rewardV, doneV):\n",
    "        # Initialise data to lists. \n",
    "        data = [{self.header[0]: stateV, \n",
    "                 self.header[1]: actonV, \n",
    "                 self.header[2]: next_stateV, \n",
    "                 self.header[3]: rewardV, \n",
    "                 self.header[4]: doneV}] \n",
    "  \n",
    "        # Creates DataFrame. \n",
    "        df = pd.DataFrame(data)\n",
    "        self.push(df)\n",
    "            \n",
    "            \n",
    "    def sample(self, batch_size=0):\n",
    "        if batch_size == 0:\n",
    "            return self.buffer\n",
    "        else:\n",
    "            return self.buffer.sample(batch_size)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.buffer.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DqnAgent(nn.Module):\n",
    "    \n",
    "    # tweak hyperparameters\n",
    "    \n",
    "    def __init__(self, ip_size = 49  , n_op=Env.ACTION_N):\n",
    "        super(DqnAgent, self).__init__()  \n",
    "        self.fc1 = nn.Linear(ip_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, 128)\n",
    "        self.fc5 = nn.Linear(128, n_op)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, Env.DIM*Env.DIM)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = Env()\n",
    "\n",
    "qvfa = DqnAgent().to(device)\n",
    "optimizer = optim.Adam(qvfa.parameters(), lr = LEARN_RATE)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "buffer = ReplayBuffer(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, ep = 0):    \n",
    "    \n",
    "    sample = random.random()    \n",
    "    #state = state.view(-1,1,Env.DIM,Env.DIM).float()\n",
    "    if sample < ep:\n",
    "        return env.sample_valid_action()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state = torch.from_numpy(state).unsqueeze(0).float().to(device)            \n",
    "            op = qvfa(state)\n",
    "            values, indices = op.max(1)\n",
    "            return indices.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mod this \n",
    "\n",
    "def optimize_model(i_episode = 0):\n",
    "    BATCH_SIZE = 3\n",
    "    if buffer.__len__() < BATCH_SIZE:\n",
    "        print(\"optimizing model Not enough samples in buffer : \",buffer.__len__())\n",
    "\n",
    "\n",
    "    transitions = buffer.sample(min(BATCH_SIZE, buffer.__len__()))    \n",
    "\n",
    "    state_batch = transitions[buffer.header[0]].values\n",
    "    state_batch = torch.from_numpy(np.stack( state_batch, axis=0 )).float().to(device)\n",
    "    #state_batch = state_batch.view(-1,1,Env.DIM,Env.DIM).float()\n",
    "\n",
    "    action_batch = torch.tensor(transitions[buffer.header[1]].values.tolist()).view(-1,1).to(device)\n",
    "\n",
    "    next_state_batch = transitions[buffer.header[2]].values\n",
    "    next_state_batch = torch.from_numpy(np.stack( next_state_batch, axis=0 )).float().to(device)\n",
    "    #next_state_batch = next_state_batch.view(-1,1,Env.DIM,Env.DIM).float()\n",
    "\n",
    "    reward_batch = torch.tensor(transitions[buffer.header[3]].values.tolist()).view(-1,1).to(device).float()\n",
    "\n",
    "    done_batch = torch.tensor(transitions[buffer.header[4]].values.tolist()).view(-1,1).to(device)\n",
    "\n",
    "    qsa = qvfa(state_batch).gather(1, action_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        qvfa.eval()\n",
    "        next_state_action_values = qvfa(next_state_batch)\n",
    "        max_next_state_values, _indices = next_state_action_values.max(dim=1)\n",
    "        max_next_state_values = max_next_state_values.view(-1,1)\n",
    "        next_state_values = ((max_next_state_values*GAMMA).float()+reward_batch).float()*(1-done_batch).float()\n",
    "        target = next_state_values.float()\n",
    "        qvfa.train()\n",
    "\n",
    "\n",
    "    # ð›¿=ð‘„(ð‘ ,ð‘Ž)âˆ’(ð‘Ÿ+ð›¾maxð‘Žð‘„(ð‘ â€²,ð‘Ž))\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(qsa, target)\n",
    "    loss.backward()\n",
    "    # for param in qvfa.parameters():param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    #print(\"loss \",loss.item())\n",
    "    writer.add_scalar(tag_loss, loss.item(), i_episode)     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mod this \n",
    "\n",
    "for i_episode in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:        \n",
    "        action = select_action(state, ep = EPSILON)\n",
    "        next_state, reward, done = env.step(action)        \n",
    "        \n",
    "        if done:\n",
    "            on_board = env.get_count()\n",
    "            if on_board  == 1:\n",
    "                # game success\n",
    "                reward = reward*100\n",
    "            else:\n",
    "                # wrong solution\n",
    "                # no reward\n",
    "                reward = -reward\n",
    "                \n",
    "        total_reward += reward\n",
    "        buffer.insert(state, action, next_state, reward, done)\n",
    "        state = next_state        \n",
    "        \n",
    "    writer.add_scalar(tag_reward, total_reward, i_episode)\n",
    "    writer.add_scalar(tag_ep, EPSILON, i_episode)\n",
    "    \n",
    "    optimize_model(i_episode)\n",
    "    \n",
    "    if i_episode%100 == 0 and i_episode > BATCH_SIZE:\n",
    "        EPSILON *= EDECAY\n",
    "    '''\n",
    "    if EPSILON >= 0.1 and i_episode > BATCH_SIZE and total_reward > MINREWARD:        \n",
    "        EPSILON -= ESUB\n",
    "        MINREWARD += MINREWARD_INCREMENT\n",
    "        if EPSILON == 0:\n",
    "            EPSILON = EMIN\n",
    "    '''\n",
    "        \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Idea: \n",
    "    Dont use nn, or FA, use state action dict.\n",
    "    For every dead end award negetive reward.\n",
    "    Use forward view TD Lambda to update the values.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
