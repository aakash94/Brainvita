{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Env import Env\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment='__')\n",
    "tag_reward = \"reward\"\n",
    "tag_loss = \"loss\"\n",
    "tag_ep = \"epsilon\"\n",
    "r_buff_header = ['state', 'action', 'next_state', 'reward', 'done']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA      = 0.9 # discount factor\n",
    "EPSILON    = 1\n",
    "ESUB       = 0.01\n",
    "EMIN       = 0.025\n",
    "LEARN_RATE = 0.001\n",
    "EDECAY = 0.99\n",
    "\n",
    "STATE_N  = Env.DIM * Env.DIM\n",
    "ACTION_N = Env.ACTION_N\n",
    "\n",
    "NUM_EPISODES = 100000\n",
    "\n",
    "MINREWARD = 25\n",
    "MINREWARD_INCREMENT = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.header = r_buff_header\n",
    "        self.buffer = pd.DataFrame(columns=self.header)        \n",
    "\n",
    "    def push(self, df_row):\n",
    "        if self.__len__() == self.capacity:\n",
    "            # Probably exceeded capacity\n",
    "            #remove a row (probably 1st one) here \n",
    "            self.buffer = self.buffer.iloc[1:]\n",
    "        #add to dataframe here\n",
    "        self.buffer = pd.concat([self.buffer, df_row])\n",
    "        \n",
    "        \n",
    "    def insert(self, stateV, actonV, next_stateV, rewardV, doneV):\n",
    "        # Initialise data to lists. \n",
    "        data = [{self.header[0]: stateV, \n",
    "                 self.header[1]: actonV, \n",
    "                 self.header[2]: next_stateV, \n",
    "                 self.header[3]: rewardV, \n",
    "                 self.header[4]: doneV}] \n",
    "  \n",
    "        # Creates DataFrame. \n",
    "        df = pd.DataFrame(data)\n",
    "        self.push(df)\n",
    "            \n",
    "            \n",
    "    def sample(self, batch_size=0):\n",
    "        if batch_size == 0:\n",
    "            return self.buffer\n",
    "        else:\n",
    "            return self.buffer.sample(batch_size)\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.buffer.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DqnAgent(nn.Module):\n",
    "    \n",
    "    # tweak hyperparameters\n",
    "    \n",
    "    def __init__(self, ip_size = 49  , n_op=Env.ACTION_N):\n",
    "        super(DqnAgent, self).__init__()  \n",
    "        self.fc1 = nn.Linear(ip_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, 128)\n",
    "        self.fc5 = nn.Linear(128, n_op)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, Env.DIM*Env.DIM)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = Env()\n",
    "\n",
    "qvfa = DqnAgent().to(device)\n",
    "optimizer = optim.Adam(qvfa.parameters(), lr = LEARN_RATE)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "buffer = ReplayBuffer(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, ep = 0):    \n",
    "    \n",
    "    sample = random.random()    \n",
    "    #state = state.view(-1,1,Env.DIM,Env.DIM).float()\n",
    "    if sample < ep:\n",
    "        return env.sample_valid_action()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state = torch.from_numpy(state).unsqueeze(0).float().to(device)            \n",
    "            op = qvfa(state)\n",
    "            values, indices = op.max(1)\n",
    "            return indices.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mod this \n",
    "\n",
    "def optimize_model(i_episode = 0):\n",
    "    BATCH_SIZE = 3\n",
    "    if buffer.__len__() < BATCH_SIZE:\n",
    "        print(\"optimizing model Not enough samples in buffer : \",buffer.__len__())\n",
    "\n",
    "\n",
    "    transitions = buffer.sample(min(BATCH_SIZE, buffer.__len__()))    \n",
    "\n",
    "    state_batch = transitions[buffer.header[0]].values\n",
    "    state_batch = torch.from_numpy(np.stack( state_batch, axis=0 )).float().to(device)\n",
    "    #state_batch = state_batch.view(-1,1,Env.DIM,Env.DIM).float()\n",
    "\n",
    "    action_batch = torch.tensor(transitions[buffer.header[1]].values.tolist()).view(-1,1).to(device)\n",
    "\n",
    "    next_state_batch = transitions[buffer.header[2]].values\n",
    "    next_state_batch = torch.from_numpy(np.stack( next_state_batch, axis=0 )).float().to(device)\n",
    "    #next_state_batch = next_state_batch.view(-1,1,Env.DIM,Env.DIM).float()\n",
    "\n",
    "    reward_batch = torch.tensor(transitions[buffer.header[3]].values.tolist()).view(-1,1).to(device).float()\n",
    "\n",
    "    done_batch = torch.tensor(transitions[buffer.header[4]].values.tolist()).view(-1,1).to(device)\n",
    "\n",
    "    qsa = qvfa(state_batch).gather(1, action_batch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        qvfa.eval()\n",
    "        next_state_action_values = qvfa(next_state_batch)\n",
    "        max_next_state_values, _indices = next_state_action_values.max(dim=1)\n",
    "        max_next_state_values = max_next_state_values.view(-1,1)\n",
    "        next_state_values = ((max_next_state_values*GAMMA).float()+reward_batch).float()*(1-done_batch).float()\n",
    "        target = next_state_values.float()\n",
    "        qvfa.train()\n",
    "\n",
    "\n",
    "    # 𝛿=𝑄(𝑠,𝑎)−(𝑟+𝛾max𝑎𝑄(𝑠′,𝑎))\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(qsa, target)\n",
    "    loss.backward()\n",
    "    # for param in qvfa.parameters():param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    #print(\"loss \",loss.item())\n",
    "    writer.add_scalar(tag_loss, loss.item(), i_episode)     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Mod this \n",
    "\n",
    "for i_episode in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:        \n",
    "        action = select_action(state, ep = EPSILON)\n",
    "        next_state, reward, done = env.step(action)        \n",
    "        \n",
    "        if done:\n",
    "            on_board = env.get_count()\n",
    "            if on_board  == 1:\n",
    "                # game success\n",
    "                reward = reward*100\n",
    "            else:\n",
    "                # wrong solution\n",
    "                # no reward\n",
    "                reward = -reward\n",
    "                \n",
    "        total_reward += reward\n",
    "        buffer.insert(state, action, next_state, reward, done)\n",
    "        state = next_state        \n",
    "        \n",
    "    writer.add_scalar(tag_reward, total_reward, i_episode)\n",
    "    writer.add_scalar(tag_ep, EPSILON, i_episode)\n",
    "    \n",
    "    optimize_model(i_episode)\n",
    "    \n",
    "    if i_episode%100 == 0 and i_episode > BATCH_SIZE:\n",
    "        EPSILON *= EDECAY\n",
    "    '''\n",
    "    if EPSILON >= 0.1 and i_episode > BATCH_SIZE and total_reward > MINREWARD:        \n",
    "        EPSILON -= ESUB\n",
    "        MINREWARD += MINREWARD_INCREMENT\n",
    "        if EPSILON == 0:\n",
    "            EPSILON = EMIN\n",
    "    '''\n",
    "        \n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Idea: \n",
    "    Dont use nn, or FA, use state action dict.\n",
    "    For every dead end award negetive reward.\n",
    "    Use forward view TD Lambda to update the values.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
